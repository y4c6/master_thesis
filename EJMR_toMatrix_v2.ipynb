{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQwDaRyQMAzqVpRARPqY2F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/y4c6/master_thesis/blob/main/EJMR_toMatrix_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMQvLUyFN6Cq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# directory\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seTDx0I3OTie",
        "outputId": "e1b50eec-bbee-46b8-adb8-eb3d64e07d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/論文相關材料/EJMRpost_1_10.json', 'r') as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "UBL05PByZIix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.keys()) #prints keys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TScJEuvT3WRr",
        "outputId": "cd1339c1-2a5d-42e2-b0b5-8856aebd3912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['topic', 'posts'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['topic'][:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCOsAjpHqWuJ",
        "outputId": "62dd1191-2568-4b97-cee7-2f573f538e5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://www.econjobrumors.com/topic/rbb-or-kari',\n",
              " 'https://www.econjobrumors.com/topic/as-ian-americans-grapple-with-tide-of-attacks-%e2%80%98we-need-our-safety-back%e2%80%99',\n",
              " 'https://www.econjobrumors.com/topic/russia-china-relations-are-strengthening',\n",
              " 'https://www.econjobrumors.com/topic/just-found-out-that-its-possible-to-major-in-human-resources',\n",
              " 'https://www.econjobrumors.com/topic/karen-and-zhang-get-married']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## asian-related posts selected by topic"
      ],
      "metadata": {
        "id": "IG4IhBneX8Tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asian_related = []\n",
        "for i in range(len(data['topic'])):\n",
        "    if 'asian' in data['topic'][i]:\n",
        "      asian_related.append(1)\n",
        "    else:\n",
        "      asian_related.append(0)\n",
        "        \n",
        "asian_related[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2nuI-Qy7GKd",
        "outputId": "6ffc1aff-1236-443c-d98c-224686b86298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asian_related.count(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO9ep-lZrINs",
        "outputId": "72766ea1-e79f-4fd9-9ae2-9f89aef2ab9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "294"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## words counts"
      ],
      "metadata": {
        "id": "gqsiZuU7YHWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['posts'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYE6-GMkRn2g",
        "outputId": "43dc7229-d612-4e30-92e1-726a9d259a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Whose more of a dishonest cringe dooshbag?',\n",
              " 'I love Karl. He’s very honest',\n",
              " 'I love Karl. He’s very honest',\n",
              " 'Perhaps your perception of his honesty is biased by your homo feelings of love for him, yes?',\n",
              " 'Brunet the journalist went into hiding the second actual world events dominated the conversation because now no one on EJMR cares about tweet fights with woke VLRM academics',\n",
              " 'Why the RBB abbreviation? Is it Rus Bed & Breakfast?',\n",
              " 'Why the RBB abbreviation? Is it Rus Bed & Breakfast?',\n",
              " 'I have absolutely no idea, but that is how he is referred to here.  I just assumed it somehow made sense to his lQ = 60 fan base.',\n",
              " 'I prefer to refer to him as RusTardFest.',\n",
              " 'Karlito >>>>>>>> rbb',\n",
              " 'Karl is a dvmb ngr',\n",
              " 'RBB is a bro <3',\n",
              " 'both are !d!ots',\n",
              " 'both are !d!ots',\n",
              " 'This',\n",
              " 's/t/f/u chad',\n",
              " 'RBB and Karl is the reason why I come here. I love ejmr characters',\n",
              " 'RBB >>>> Karl.',\n",
              " 'RBB has awful taste, but is a bro.',\n",
              " 'Why are they at odds?']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt') #this is download for tonkenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpSYjI5YhK5u",
        "outputId": "0c68ac82-fd27-4fab-e873-601f553f9cb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "posts = [' '.join(i).lower() for i in data['posts']]"
      ],
      "metadata": {
        "id": "eMkayaVdOpQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(posts), len(posts))\n",
        "posts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "-fqEAFGZRxHk",
        "outputId": "ae439c32-eace-4dc1-a434-e83241345482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 80000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'whose more of a dishonest cringe dooshbag? i love karl. he’s very honest i love karl. he’s very honest perhaps your perception of his honesty is biased by your homo feelings of love for him, yes? brunet the journalist went into hiding the second actual world events dominated the conversation because now no one on ejmr cares about tweet fights with woke vlrm academics why the rbb abbreviation? is it rus bed & breakfast? why the rbb abbreviation? is it rus bed & breakfast? i have absolutely no idea, but that is how he is referred to here.  i just assumed it somehow made sense to his lq = 60 fan base. i prefer to refer to him as rustardfest. karlito >>>>>>>> rbb karl is a dvmb ngr rbb is a bro <3 both are !d!ots both are !d!ots this s/t/f/u chad rbb and karl is the reason why i come here. i love ejmr characters rbb >>>> karl. rbb has awful taste, but is a bro. why are they at odds?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "example_sent = posts[0]\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_tokens = word_tokenize(example_sent)\n",
        "\n",
        "# converts the words in word_tokens to lower case and then checks whether\n",
        "#they are present in stop_words or not\n",
        "\n",
        "filtered_sentence = [w.translate(str.maketrans('', '', string.punctuation)) for w in word_tokens if not w.lower() in stop_words]\n",
        "filtered_sentence = list(filter(None, filtered_sentence))\n",
        "\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m_UrwQFfaPb",
        "outputId": "3b779fcf-63a8-4f60-f236-189db3338e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['whose', 'more', 'of', 'a', 'dishonest', 'cringe', 'dooshbag', '?', 'i', 'love', 'karl', '.', 'he', '’', 's', 'very', 'honest', 'i', 'love', 'karl', '.', 'he', '’', 's', 'very', 'honest', 'perhaps', 'your', 'perception', 'of', 'his', 'honesty', 'is', 'biased', 'by', 'your', 'homo', 'feelings', 'of', 'love', 'for', 'him', ',', 'yes', '?', 'brunet', 'the', 'journalist', 'went', 'into', 'hiding', 'the', 'second', 'actual', 'world', 'events', 'dominated', 'the', 'conversation', 'because', 'now', 'no', 'one', 'on', 'ejmr', 'cares', 'about', 'tweet', 'fights', 'with', 'woke', 'vlrm', 'academics', 'why', 'the', 'rbb', 'abbreviation', '?', 'is', 'it', 'rus', 'bed', '&', 'breakfast', '?', 'why', 'the', 'rbb', 'abbreviation', '?', 'is', 'it', 'rus', 'bed', '&', 'breakfast', '?', 'i', 'have', 'absolutely', 'no', 'idea', ',', 'but', 'that', 'is', 'how', 'he', 'is', 'referred', 'to', 'here', '.', 'i', 'just', 'assumed', 'it', 'somehow', 'made', 'sense', 'to', 'his', 'lq', '=', '60', 'fan', 'base', '.', 'i', 'prefer', 'to', 'refer', 'to', 'him', 'as', 'rustardfest', '.', 'karlito', '>', '>', '>', '>', '>', '>', '>', '>', 'rbb', 'karl', 'is', 'a', 'dvmb', 'ngr', 'rbb', 'is', 'a', 'bro', '<', '3', 'both', 'are', '!', 'd', '!', 'ots', 'both', 'are', '!', 'd', '!', 'ots', 'this', 's/t/f/u', 'chad', 'rbb', 'and', 'karl', 'is', 'the', 'reason', 'why', 'i', 'come', 'here', '.', 'i', 'love', 'ejmr', 'characters', 'rbb', '>', '>', '>', '>', 'karl', '.', 'rbb', 'has', 'awful', 'taste', ',', 'but', 'is', 'a', 'bro', '.', 'why', 'are', 'they', 'at', 'odds', '?']\n",
            "['whose', 'dishonest', 'cringe', 'dooshbag', 'love', 'karl', '’', 'honest', 'love', 'karl', '’', 'honest', 'perhaps', 'perception', 'honesty', 'biased', 'homo', 'feelings', 'love', 'yes', 'brunet', 'journalist', 'went', 'hiding', 'second', 'actual', 'world', 'events', 'dominated', 'conversation', 'one', 'ejmr', 'cares', 'tweet', 'fights', 'woke', 'vlrm', 'academics', 'rbb', 'abbreviation', 'rus', 'bed', 'breakfast', 'rbb', 'abbreviation', 'rus', 'bed', 'breakfast', 'absolutely', 'idea', 'referred', 'assumed', 'somehow', 'made', 'sense', 'lq', '60', 'fan', 'base', 'prefer', 'refer', 'rustardfest', 'karlito', 'rbb', 'karl', 'dvmb', 'ngr', 'rbb', 'bro', '3', 'ots', 'ots', 'stfu', 'chad', 'rbb', 'karl', 'reason', 'come', 'love', 'ejmr', 'characters', 'rbb', 'karl', 'rbb', 'awful', 'taste', 'bro', 'odds']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_posts = ' '.join(posts)"
      ],
      "metadata": {
        "id": "uGc_qJopTJL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(all_posts), type(all_posts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9Rr7EJDTRdn",
        "outputId": "c22c0494-582d-412c-845b-9b6ac00e9500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140456811 <class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def clean_article(article):\n",
        "  # Lowercase all characters\n",
        "  article = article.lower()\n",
        "\n",
        "  # Remove URLs\n",
        "  article = re.sub(r'https?://\\S+', '', article)\n",
        "\n",
        "  # Remove digits\n",
        "  article = re.sub(r'\\d+', '', article)\n",
        "\n",
        "  # Remove punctuation\n",
        "  article = article.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  # Tokenize the article\n",
        "  article_tokens = word_tokenize(article)\n",
        "\n",
        "  # Remove stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  filtered_article_tokens = [token for token in article_tokens if token not in stop_words]\n",
        "\n",
        "  return ' '.join(filtered_article_tokens)\n",
        "\n",
        "# Example usage\n",
        "article = \"This is an example article. It contains some URLs (https://www.example.com) and digits (123456). We want to clean it by lowercasing all characters, removing stopwords, punctuation, URLs, and digits.\"\n",
        "\n",
        "cleaned_article = clean_article(all_posts)\n",
        "print(cleaned_article[:100])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQQc_ur7E0Gw",
        "outputId": "e87e7b15-5d80-4ab2-f6a2-c1cc3924b751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "whose dishonest cringe dooshbag love karl ’ honest love karl ’ honest perhaps perception honesty bia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# split() returns list of all the words in the string\n",
        "split_it = cleaned_article.split()\n",
        "\n",
        "# Pass the split_it list to instance of Counter class.\n",
        "counter = Counter(split_it)\n",
        "  \n",
        "# most_common() produces k frequently encountered\n",
        "# input values and their respective counts.\n",
        "most_occur = counter.most_common(10000)"
      ],
      "metadata": {
        "id": "cX0cuIiCVAhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def clean_and_count_words(article):\n",
        "  # Lowercase all characters\n",
        "  article = article.lower()\n",
        "\n",
        "  # Remove URLs\n",
        "  article = re.sub(r'https?://\\S+', '', article)\n",
        "\n",
        "  # Remove digits\n",
        "  article = re.sub(r'\\d', '', article)\n",
        "\n",
        "  # Remove punctuation\n",
        "  article = article.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  # Tokenize the article\n",
        "  article_tokens = word_tokenize(article)\n",
        "\n",
        "  # Remove stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  filtered_article_tokens = [token for token in article_tokens if token not in stop_words]\n",
        "\n",
        "  # Count the frequency of each word\n",
        "  word_counts = Counter(filtered_article_tokens)\n",
        "\n",
        "  return word_counts\n",
        "\n",
        "# Example usage\n",
        "article = \"This is an example article. It contains some URLs (https://www.example.com) and digits (123456). We want to clean it by lowercasing all characters, removing stopwords, punctuation, URLs, and digits.\"\n",
        "\n",
        "word_counts = clean_and_count_words(article)\n",
        "print(word_counts)\n"
      ],
      "metadata": {
        "id": "cOH9ZQGAIAdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_threshold = 10000\n",
        "{x: count for x, count in counter.items() if count >= min_threshold}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vws27nr4XBtR",
        "outputId": "98e46e5a-70b1-442c-b6b2-c9bb80d7aea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'’': 137701,\n",
              " 'yes': 20643,\n",
              " 'world': 19748,\n",
              " 'one': 61913,\n",
              " 'ejmr': 13849,\n",
              " 'idea': 10000,\n",
              " 'made': 12274,\n",
              " 'come': 12017,\n",
              " 'much': 38711,\n",
              " 'find': 15020,\n",
              " 'many': 32276,\n",
              " 'american': 10985,\n",
              " '“': 26152,\n",
              " 'said': 16203,\n",
              " 'want': 33186,\n",
              " 'every': 16665,\n",
              " '”': 26106,\n",
              " 'last': 14419,\n",
              " 'china': 15687,\n",
              " 'years': 36646,\n",
              " 'high': 16730,\n",
              " 'school': 25973,\n",
              " 'people': 85254,\n",
              " 'also': 39741,\n",
              " 'look': 17697,\n",
              " 'left': 10288,\n",
              " 'right': 23384,\n",
              " 'see': 25266,\n",
              " 'anyone': 13517,\n",
              " 'might': 13072,\n",
              " 'really': 31385,\n",
              " 'end': 11441,\n",
              " 'ever': 11004,\n",
              " 'think': 48491,\n",
              " 'maybe': 15192,\n",
              " 'true': 15771,\n",
              " 'would': 70104,\n",
              " 'never': 23088,\n",
              " 'well': 27751,\n",
              " 'op': 25942,\n",
              " 'russia': 11495,\n",
              " 'already': 12808,\n",
              " 'isnt': 10837,\n",
              " 'new': 19902,\n",
              " 'chinese': 10266,\n",
              " 'state': 12526,\n",
              " 'go': 33467,\n",
              " 'good': 56932,\n",
              " 'may': 12586,\n",
              " 'even': 46464,\n",
              " 'cant': 15378,\n",
              " 'without': 13588,\n",
              " 'thing': 15017,\n",
              " 'didnt': 11851,\n",
              " 'anything': 13710,\n",
              " 'time': 39678,\n",
              " 'things': 16507,\n",
              " 'money': 16763,\n",
              " 'like': 85772,\n",
              " 'know': 42655,\n",
              " 'pay': 13150,\n",
              " 'back': 20129,\n",
              " 'could': 23681,\n",
              " 'way': 25545,\n",
              " 'im': 27973,\n",
              " 'sure': 17689,\n",
              " 'dont': 57449,\n",
              " 'give': 13036,\n",
              " 'better': 30736,\n",
              " 'next': 10466,\n",
              " 'always': 12720,\n",
              " 'going': 25023,\n",
              " 'two': 18252,\n",
              " 'actually': 19275,\n",
              " 'us': 44601,\n",
              " 'working': 10890,\n",
              " 'let': 10827,\n",
              " 'lot': 18513,\n",
              " 'probably': 16692,\n",
              " 'pretty': 13149,\n",
              " 'best': 17706,\n",
              " 'economics': 15882,\n",
              " 'someone': 16565,\n",
              " 'guy': 14137,\n",
              " 'long': 12910,\n",
              " 'year': 31524,\n",
              " 'work': 35171,\n",
              " 'got': 22214,\n",
              " 'still': 26373,\n",
              " 'papers': 16417,\n",
              " 'get': 67720,\n",
              " 'paper': 20952,\n",
              " 'problem': 13005,\n",
              " 'care': 11732,\n",
              " 'day': 12863,\n",
              " 'must': 10623,\n",
              " 'since': 12679,\n",
              " 'understand': 11238,\n",
              " 'thats': 19805,\n",
              " 'finance': 14639,\n",
              " 'something': 18217,\n",
              " 'bad': 18377,\n",
              " 'low': 10966,\n",
              " 'make': 31018,\n",
              " 'top': 35585,\n",
              " 'phd': 26126,\n",
              " 'econ': 19637,\n",
              " 'market': 20734,\n",
              " 'first': 20695,\n",
              " 'youre': 18185,\n",
              " 'big': 12259,\n",
              " 'k': 20636,\n",
              " 'hard': 12240,\n",
              " 'case': 10174,\n",
              " 'different': 12033,\n",
              " 'enough': 13295,\n",
              " 'job': 28073,\n",
              " 'data': 14211,\n",
              " 'everyone': 12527,\n",
              " 'take': 21225,\n",
              " 'question': 10561,\n",
              " 'covid': 10554,\n",
              " 'read': 12619,\n",
              " 'doesnt': 15190,\n",
              " 'mean': 13986,\n",
              " 'use': 14504,\n",
              " 'women': 19939,\n",
              " 'try': 10538,\n",
              " 'math': 10249,\n",
              " 'around': 12914,\n",
              " 'point': 16878,\n",
              " 'lol': 19675,\n",
              " 'nothing': 14737,\n",
              " 'wrong': 11848,\n",
              " 'feel': 10324,\n",
              " 'country': 15560,\n",
              " 'seems': 13923,\n",
              " 'least': 13651,\n",
              " 'another': 12534,\n",
              " 'schools': 16291,\n",
              " 'say': 23742,\n",
              " 'almost': 10602,\n",
              " 'place': 12366,\n",
              " 'life': 20670,\n",
              " 'students': 21259,\n",
              " 'student': 10853,\n",
              " 'guys': 10964,\n",
              " 'research': 22847,\n",
              " 'department': 10066,\n",
              " 'great': 18044,\n",
              " 'need': 26123,\n",
              " 'university': 15430,\n",
              " 'etc': 15240,\n",
              " 'course': 10253,\n",
              " 'post': 13872,\n",
              " 'though': 12733,\n",
              " 'live': 11453,\n",
              " 'men': 11486,\n",
              " 'makes': 10845,\n",
              " 'real': 16927,\n",
              " 'getting': 14292,\n",
              " 'person': 12748,\n",
              " 'theory': 10675,\n",
              " 'white': 10069,\n",
              " 'kids': 11086,\n",
              " 'old': 10034,\n",
              " 'less': 14114,\n",
              " 'tenure': 11077}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Define the list of text documents\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "# Use the Counter class to count the occurrences of each word in the documents\n",
        "word_counts = [Counter(document.split()) for document in documents]\n",
        "\n",
        "# Initialize the CountVectorizer with the vocabulary we want to use\n",
        "vectorizer = CountVectorizer(vocabulary=[\"this\", \"is\", \"the\", \"first\", \"document\"])\n",
        "\n",
        "# Transform the word counts into a numerical feature vector\n",
        "features = vectorizer.fit_transform(word_counts).toarray()\n",
        "\n",
        "print(features)\n"
      ],
      "metadata": {
        "id": "3JMlB5o5HrEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Out of memory\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(\n",
        "    lowercase = True, \n",
        "    analyzer = 'word',\n",
        "    stop_words = 'english',\n",
        "    ngram_range = (1, 1),\n",
        "    max_features = 100000\n",
        "    )\n",
        "X = vectorizer.fit_transform(posts)\n",
        "X = X.toarray()"
      ],
      "metadata": {
        "id": "qK9cXISuZz4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LassoCV\n",
        "\n",
        "clf = LassoCV()\n",
        "clf.fit(X, np.array(asian_post))\n",
        "print(clf.coef_[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfYnuyiedo1P",
        "outputId": "c008c2f6-3461-43c7-f888-7546b7c78b46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0. -0.  0.  0.  0. -0. -0. -0. -0. -0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/gdrive/MyDrive/論文相關材料/EJMRpost.json\", \"w\") as outfile:\n",
        "    #json.dump(EJMRpost, outfile)"
      ],
      "metadata": {
        "id": "eF7klXAHOmoi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}