{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvlHUzoeeV6jB+hdz8g30S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/y4c6/master_thesis/blob/main/EJMR_toMatrix_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMQvLUyFN6Cq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# directory\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seTDx0I3OTie",
        "outputId": "7fb52c31-15d0-48f6-e524-f5e0953bf0e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/論文相關材料/EJMRpost_1_10.json', 'r') as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "UBL05PByZIix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.keys()) #prints keys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TScJEuvT3WRr",
        "outputId": "cd1339c1-2a5d-42e2-b0b5-8856aebd3912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['topic', 'posts'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['topic'][:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCOsAjpHqWuJ",
        "outputId": "5d587f7f-7d90-437d-c741-ee74558d6c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://www.econjobrumors.com/topic/rbb-or-kari',\n",
              " 'https://www.econjobrumors.com/topic/as-ian-americans-grapple-with-tide-of-attacks-%e2%80%98we-need-our-safety-back%e2%80%99',\n",
              " 'https://www.econjobrumors.com/topic/russia-china-relations-are-strengthening',\n",
              " 'https://www.econjobrumors.com/topic/just-found-out-that-its-possible-to-major-in-human-resources',\n",
              " 'https://www.econjobrumors.com/topic/karen-and-zhang-get-married']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## asian-related posts selected by topic"
      ],
      "metadata": {
        "id": "IG4IhBneX8Tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asian_target = ['asian', 'asia', 'china', 'chinese', 'korea', 'korean', 'japan', 'japanese', 'taiwan', 'taiwanese', 'east']\n",
        "\n",
        "\n",
        "\n",
        "# initialize an empty list to store the indexes\n",
        "asian_topic_idx = []\n",
        "\n",
        "# iterate over the topics list\n",
        "for i, topic in enumerate(data['topic']):\n",
        "  # check if any of the target words are in the current topic\n",
        "  if any(word in topic for word in asian_target):\n",
        "    # if the target word is found, append the index to the indexes list\n",
        "    asian_topic_idx.append(i)\n",
        "\n",
        "print(asian_topic_idx[:15])\n",
        "'''\n",
        "for i in range(len(data['topic'])):\n",
        "  for j in asian_target:\n",
        "    if j in data['topic'][i]:\n",
        "      asian_related.append(1)\n",
        "    else:\n",
        "      asian_related.append(0)\n",
        "'''        \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "H2nuI-Qy7GKd",
        "outputId": "3d5ebea5-41d1-4e44-b456-0c40fc8c1ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 23, 44, 69, 79, 110, 129, 154, 158, 215, 232, 243, 265, 283, 288]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfor i in range(len(data['topic'])):\\n  for j in asian_target:\\n    if j in data['topic'][i]:\\n      asian_related.append(1)\\n    else:\\n      asian_related.append(0)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(asian_topic_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8JJBRZPPa2h",
        "outputId": "0a29fa24-ecf3-4151-8896-f871783d175a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2597"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asian_topic_idx_01 = np.zeros(max(asian_topic_idx)+1)\n",
        "asian_topic_idx_01[asian_topic_idx] = 1\n",
        "asian_topic_idx_01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO9ep-lZrINs",
        "outputId": "afc17633-0385-4e59-e037-73b6141bf47d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., ..., 0., 0., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_indices(list_to_check, item_to_find):\n",
        "    indices = []\n",
        "    for idx, value in enumerate(list_to_check):\n",
        "        if value == item_to_find:\n",
        "            indices.append(idx)\n",
        "    return indices\n",
        "\n",
        "asian_topic_idx = find_indices(asian_related, 1)\n",
        "asian_topic_idx[-10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPJwQ3pqXFx6",
        "outputId": "9aa3b050-b263-496b-edb1-0a125702ff4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "77989"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(list( dict.fromkeys(asian_topic_idx) )) # to see if there is duplicated posts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU7I9B530Slf",
        "outputId": "af02f60c-6f1d-4188-bc6c-70e89f89f41c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "294"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(asian_topic_idx) / len( data['posts']) # to see the ratio "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQFtXWFT1r_o",
        "outputId": "ce9e2006-eba5-4011-d4bc-0ec2a02f6055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0324625"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "filtered out asian-posts"
      ],
      "metadata": {
        "id": "aWjs7PYCXpzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asian_posts = [data['posts'][i] for i in asian_topic_idx]"
      ],
      "metadata": {
        "id": "2a0Rm5pdXyh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "word counts for asian_posts"
      ],
      "metadata": {
        "id": "sfnheln9aw5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "posts = [' '.join(i).lower() for i in asian_posts]"
      ],
      "metadata": {
        "id": "ukl2ZQaeaznO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(posts), len(posts))\n",
        "posts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "lYlzq15Ha7nF",
        "outputId": "e989df68-cda3-439f-b004-d372ec671843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 294\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"not even accounting for indians. all the quant clubs at universities are majority asian, all majoring in computational finance / math / cs / financial engineering. is this the only way out if one majors in math and doesn't want to join academia or be a high school teacher? yeah, nvidia is more than 50% asian. i assume their engineering teams are 75% asian. chinese are in all the quant finance programs.\\nso original. chinese are in all the quant finance programs.\\nso original. your “criticism” shows how origjnal you are, mor/on. fundamental is better than quant these days, needs more creativity and pays more over time. chinese are in all the quant finance programs.\\nso original. aren't you in quant? fundamental is better than quant these days, needs more creativity and pays more over time. define fundamental equity long/short, usually trading around earnings beats/misses. fundamental is better than quant these days, needs more creativity and pays more over time. stats or gtfo it's much easier for fundamental people to point to names they picked and more common to get an explicit pnl cut. also 90% of new fund launches are fundamental, with much lower barriers to get started. equity long/short, usually trading around earnings beats/misses. those words are meaningless, what's your var, you use quant for that. it's much easier for fundamental people to point to names they picked and more common to get an explicit pnl cut. also 90% of new fund launches are fundamental, with much lower barriers to get started. it's much easier just to throw a dart at a dartboard of names. lol at var, anyone can do that, there is commercial software for it these days. lol at var, anyone can do that, there is commercial software for it these days. you salesman, nothing wrong with that. quants are on the backend to make sure during a crises you don't lose your shirt. well you said it, backend. backend means lower pay. the work quants do is pretty commoditized now, unless they are a pm with their own book. not true for a fundamental guy. yeah, nvidia is more than 50% asian.\\ni assume their engineering teams are 75% asian. source or gtfo. well you said it, backend. backend means lower pay. the work quants do is pretty commoditized now, unless they are a pm with their own book. not true for a fundamental guy. during good times sure, what about during bad times and you are in the red? not even accounting for indians.\\nall the quant clubs at universities are majority asian, all majoring in computational finance / math / cs / financial engineering.\\nis this the only way out if one majors in math and doesn't want to join academia or be a high school teacher? cryptography is much harder than quant. check about full homomorphic encryption. if you are in the red too far or too long, everyone gets fired.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create a matrix using CountVectorizer\n",
        "vectorizer = CountVectorizer(preprocessor = preprocess_text, \n",
        "                tokenizer = tokenize_and_remove_stopwords, \n",
        "                max_features = 10000)"
      ],
      "metadata": {
        "id": "ihTPzdR8bJBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts_matrix = vectorizer.fit_transform(posts)\n",
        "\n",
        "# Print the matrix\n",
        "print(posts_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvqnEp13bTiI",
        "outputId": "3bea60d4-4afd-44b3-d56f-4beb66babc71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## words counts"
      ],
      "metadata": {
        "id": "gqsiZuU7YHWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['posts'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYE6-GMkRn2g",
        "outputId": "43dc7229-d612-4e30-92e1-726a9d259a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Whose more of a dishonest cringe dooshbag?',\n",
              " 'I love Karl. He’s very honest',\n",
              " 'I love Karl. He’s very honest',\n",
              " 'Perhaps your perception of his honesty is biased by your homo feelings of love for him, yes?',\n",
              " 'Brunet the journalist went into hiding the second actual world events dominated the conversation because now no one on EJMR cares about tweet fights with woke VLRM academics',\n",
              " 'Why the RBB abbreviation? Is it Rus Bed & Breakfast?',\n",
              " 'Why the RBB abbreviation? Is it Rus Bed & Breakfast?',\n",
              " 'I have absolutely no idea, but that is how he is referred to here.  I just assumed it somehow made sense to his lQ = 60 fan base.',\n",
              " 'I prefer to refer to him as RusTardFest.',\n",
              " 'Karlito >>>>>>>> rbb',\n",
              " 'Karl is a dvmb ngr',\n",
              " 'RBB is a bro <3',\n",
              " 'both are !d!ots',\n",
              " 'both are !d!ots',\n",
              " 'This',\n",
              " 's/t/f/u chad',\n",
              " 'RBB and Karl is the reason why I come here. I love ejmr characters',\n",
              " 'RBB >>>> Karl.',\n",
              " 'RBB has awful taste, but is a bro.',\n",
              " 'Why are they at odds?']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "posts = [' '.join(i).lower() for i in data['posts']]"
      ],
      "metadata": {
        "id": "eMkayaVdOpQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(posts), len(posts))\n",
        "posts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "-fqEAFGZRxHk",
        "outputId": "ae439c32-eace-4dc1-a434-e83241345482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 80000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'whose more of a dishonest cringe dooshbag? i love karl. he’s very honest i love karl. he’s very honest perhaps your perception of his honesty is biased by your homo feelings of love for him, yes? brunet the journalist went into hiding the second actual world events dominated the conversation because now no one on ejmr cares about tweet fights with woke vlrm academics why the rbb abbreviation? is it rus bed & breakfast? why the rbb abbreviation? is it rus bed & breakfast? i have absolutely no idea, but that is how he is referred to here.  i just assumed it somehow made sense to his lq = 60 fan base. i prefer to refer to him as rustardfest. karlito >>>>>>>> rbb karl is a dvmb ngr rbb is a bro <3 both are !d!ots both are !d!ots this s/t/f/u chad rbb and karl is the reason why i come here. i love ejmr characters rbb >>>> karl. rbb has awful taste, but is a bro. why are they at odds?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_posts = ' '.join(posts)"
      ],
      "metadata": {
        "id": "uGc_qJopTJL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(all_posts), type(all_posts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9Rr7EJDTRdn",
        "outputId": "bae49816-ed12-4d46-f613-f9241181596c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140456842 <class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt') #this is download for tonkenizer\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVND76iPRJ60",
        "outputId": "a3987430-16f0-4955-b246-253a06eea687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Define a function to preprocess text\n",
        "def preprocess_text(text):\n",
        "  # Lowercase all characters\n",
        "  text = text.lower()\n",
        "\n",
        "  # Remove URLs\n",
        "  text = re.sub(r'https?://\\S+', '', text)\n",
        "\n",
        "  # Remove digits\n",
        "  text = text.translate(str.maketrans('', '', string.digits))\n",
        "\n",
        "  # Remove punctuation\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  return text\n",
        "\n",
        "# Define a function to tokenize and remove stopwords from text\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "  # Tokenize the article\n",
        "  text_tokens = word_tokenize(text)\n",
        "\n",
        "  # Load English stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  # Remove stopwords\n",
        "  tokens = [token for token in text_tokens if token not in stop_words]\n",
        "\n",
        "  return tokens\n"
      ],
      "metadata": {
        "id": "cOH9ZQGAIAdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create a matrix using CountVectorizer\n",
        "vectorizer = CountVectorizer(preprocessor=preprocess_text, \n",
        "                tokenizer=tokenize_and_remove_stopwords, \n",
        "                max_features = 10000) # 80000, run out of ram"
      ],
      "metadata": {
        "id": "vs6WXx3EXtFY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "4342024f-12e2-496d-e265-ab08403f8794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6cbfefd01ebe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create a matrix using CountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m vectorizer = CountVectorizer(preprocessor=preprocess_text, \n\u001b[0m\u001b[1;32m      5\u001b[0m                 \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize_and_remove_stopwords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 max_features = 10000) # 80000, run out of ram\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocess_text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "posts_matrix = vectorizer.fit_transform(posts)\n",
        "\n",
        "# Print the matrix\n",
        "print(posts_matrix.toarray())"
      ],
      "metadata": {
        "id": "cWdK4Ul7XkWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5Su1txePe8D4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "posts_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP1yT8UX940A",
        "outputId": "736b1144-8e43-4ddc-c6eb-6efd675493b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('posts_matrix_test.pkl','wb') as f:\n",
        "  pickle.dump(pos#ts_matrix, f)"
      ],
      "metadata": {
        "id": "7zkOG3E5Ztai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [ cleaned_tokenize(posts[i]) for i in range(len(posts)) ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "8tuugDqQRF6x",
        "outputId": "9b2a6751-c18d-4d86-f7b3-927f868c9429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-e07b5b28ad39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mcleaned_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-43-e07b5b28ad39>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mcleaned_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'cleaned_tokenize' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcwbusglXwab",
        "outputId": "1bfdc429-4eb4-4d8e-ce90-ccf49233bb8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80000"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "counter = Counter(tokens)\n",
        "counter.most_common(10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "od5DjHfdNnbJ",
        "outputId": "96b4f67c-2bfa-44d7-ba82-ec8ad5fc68ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-c962e16126bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Out of memory\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(\n",
        "    lowercase = True, \n",
        "    analyzer = 'word',\n",
        "    stop_words = 'english',\n",
        "    ngram_range = (1, 1),\n",
        "    max_features = 10000\n",
        "    )\n",
        "X = vectorizer.fit_transform(tokens)\n",
        "X = X.toarray()"
      ],
      "metadata": {
        "id": "qK9cXISuZz4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LassoCV\n",
        "\n",
        "clf = LassoCV()\n",
        "clf.fit(posts_matrix, np.array([1]* len(asian_topic_idx)))\n",
        "print(clf.coef_[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfYnuyiedo1P",
        "outputId": "e16621cf-628e-43e5-ad09-95797deb461a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0, tolerance: 0.0\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.coef_[clf.coef_!=0]"
      ],
      "metadata": {
        "id": "dUcDucO4hESz",
        "outputId": "9d7142c7-3ff6-4c94-fe95-1d2385861de1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([], dtype=float64)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install doubleml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u-iKWK2lLqo",
        "outputId": "835d036e-e0a2-47b5-b720-844f37c24e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting doubleml\n",
            "  Downloading DoubleML-0.5.2-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from doubleml) (1.2.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from doubleml) (1.7.3)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.8/dist-packages (from doubleml) (0.12.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from doubleml) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from doubleml) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from doubleml) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->doubleml) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->doubleml) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->doubleml) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->doubleml) (3.1.0)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.8/dist-packages (from statsmodels->doubleml) (0.5.3)\n",
            "Installing collected packages: doubleml\n",
            "Successfully installed doubleml-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import doubleml as dml\n",
        "from sklearn.base import clone\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "learner = RandomForestRegressor(n_estimators=100, max_features=20, max_depth=5, min_samples_leaf=2)\n",
        "ml_g = learner\n",
        "ml_m = learner\n",
        "dml_plr_obj = dml.DoubleMLPLR(obj_dml_data, ml_g, ml_m)\n",
        "dml_plr_obj.fit().summary"
      ],
      "metadata": {
        "id": "BN_g-lPHltR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from doubleml.datasets import make_plr_CCDDHNR2018\n",
        "obj_dml_data = make_plr_CCDDHNR2018(alpha=0.5, n_obs=500, dim_x=20)"
      ],
      "metadata": {
        "id": "rE96FUI1mBa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(obj_dml_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6gb6Rc_mKCY",
        "outputId": "673a22c7-8c5f-431f-bba0-327816f8a422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "doubleml.double_ml_data.DoubleMLData"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    }
  ]
}