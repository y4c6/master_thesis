{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZJ0U1FMmYkwIOgoegQtGC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/y4c6/master_thesis/blob/main/EJMR_toMatrix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this file, i collect the codes for **combining the json** and **tokenizing**."
      ],
      "metadata": {
        "id": "4FMt4aiJ8bns"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4OHJm2Q8K3Y"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "# directory\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json"
      ],
      "metadata": {
        "id": "ts0CL8vt865Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare jsonfiles"
      ],
      "metadata": {
        "id": "cB1mYS-Z8_Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import json\n",
        "\n",
        "def list_json_files(path, start_with): \n",
        "    # get a list of json files that starts with specific word \n",
        "    json_files = glob.glob(path + f'/{start_with}*.json')\n",
        "    return json_files\n",
        "\n",
        "def concat_json_files(file_paths):\n",
        "    # concatenate the content of all the files in the list\n",
        "    data = {'topic':[], 'posts':[]}\n",
        "    for file_path in file_paths:\n",
        "        with open(file_path, 'r') as f:\n",
        "            file_data = json.load(f)\n",
        "            data['topic'].extend(file_data['topic'])\n",
        "            data['posts'].extend(file_data['posts'])\n",
        "    return data\n",
        "\n",
        "def concat_json_files_with_start(path, start_with):\n",
        "    json_files = list_json_files(path, start_with)\n",
        "    data = concat_json_files(json_files)\n",
        "    return data"
      ],
      "metadata": {
        "id": "sIKwls9Q9F4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/gdrive/MyDrive/論文相關材料/'\n",
        "start_with = 'EJMRpost_'\n",
        "data = concat_json_files_with_start(path, start_with)"
      ],
      "metadata": {
        "id": "W5TbkZ989Ipt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data['topic'])"
      ],
      "metadata": {
        "id": "z5FhGK9V9L35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## construct the dictionary to dataframe\n",
        "df = pd.DataFrame(data)\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "zEeTF6lM9YRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df.to_pickle(\"./ejmr_20.pkl\")"
      ],
      "metadata": {
        "id": "h-gYJL_S9eUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unpickled_df = pd.read_pickle(\"./ejmr_20.pkl\")  \n",
        "unpickled_df.head(3)"
      ],
      "metadata": {
        "id": "pt4NRKnA9gbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## select out the asian-related posts"
      ],
      "metadata": {
        "id": "_qS9VtkV9lUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asia_target = ['asian', 'asia', 'korea', 'korean', 'japan', 'japanese', 'taiwan', 'taiwanese', 'east', 'hongkong']\n",
        "china_target = ['china', 'chinese']"
      ],
      "metadata": {
        "id": "e4-gGEx39rdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining all the conditions inside a function\n",
        "def condition(x):\n",
        "    if any( word in x for word in asia_target): #series.str.contains('Mel').any()\n",
        "        return \"asia\"\n",
        "    elif any( word in x for word in china_target):\n",
        "        return \"china\"\n",
        "    else:\n",
        "        return \"other\"\n",
        " \n",
        "# Applying the conditions\n",
        "df['Target'] = df['topic'].apply(condition)\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "KqbYt76l9sE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare `y`"
      ],
      "metadata": {
        "id": "eMzIKRMv9xmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(df[df['Target'] == 'other'].index)\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "uD_7t48B924Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['y'] = df['Target'].apply(lambda x: 1 if x == 'china' else 0)\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "KUtKOTtv99DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define functions for preprocessing"
      ],
      "metadata": {
        "id": "53KW8YpI-AXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt') #this is download for tonkenizer\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "kqee27kf-D_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from langdetect import detect\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Define a function to preprocess text\n",
        "def preprocess_text(text):\n",
        "\n",
        "  # Detect the language of the text\n",
        "  language = detect(text)\n",
        "  # If the text is not in English, return an empty string\n",
        "  if language != 'en':\n",
        "    return ''\n",
        "\n",
        "  # Lowercase all characters\n",
        "  text = text.lower()\n",
        "\n",
        "  # Remove URLs\n",
        "  text = re.sub(r'https?://\\S+', '', text)\n",
        "\n",
        "  # Remove digits\n",
        "  text = text.translate(str.maketrans('', '', string.digits))\n",
        "\n",
        "  # Remove punctuation\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "# Define a function to tokenize and remove stopwords from text and stemmer\n",
        "def stemmer_tokenize_and_remove_stopwords(text):\n",
        "  # Tokenize the article\n",
        "  text_tokens = word_tokenize(text)\n",
        "\n",
        "  # Load English stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  # Remove stopwords\n",
        "  tokens = [token for token in text_tokens if token not in stop_words]\n",
        "\n",
        "  # Stem the tokens\n",
        "  stemmer = PorterStemmer()\n",
        "  tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "\n",
        "# Define a function to tokenize and remove stopwords from text and lemmatize\n",
        "def lemmatizer_tokenize_and_remove_stopwords(text):\n",
        "  # Tokenize the article\n",
        "  text_tokens = word_tokenize(text)\n",
        "\n",
        "  # Load English stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  # Remove stopwords\n",
        "  tokens = [token for token in text_tokens if token not in stop_words]\n",
        "\n",
        "  # Lemmatize the tokens\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "  return tokens\n"
      ],
      "metadata": {
        "id": "XAzu8B7V-GcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare `X`"
      ],
      "metadata": {
        "id": "1iwNm_ggAvqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## check library\n",
        "import gensim\n",
        "\n",
        "## ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# # if you want to see the training messages, you can use it\n",
        "# import logging\n",
        "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "## the input type\n",
        "df['text_tokenized'] = df['posts'].apply(lambda x: tokenize_and_remove_stopwords(preprocess_text( ' '.join(x) ))) \n",
        "df[['y', 'text_tokenized']].head(3)"
      ],
      "metadata": {
        "id": "yxEbxzu3Ax-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_pickle(\"/content/gdrive/MyDrive/Thesis_Data&Result/ejmr_20_token.pkl\")"
      ],
      "metadata": {
        "id": "YR82N4XnBEsc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}